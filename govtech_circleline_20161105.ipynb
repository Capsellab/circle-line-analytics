{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How we caught the Circle Line rogue train with data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Note: This is meant to be a companion to our post on the Data.gov.sg blog (https://blog.data.gov.sg/how-we-caught-the-circle-line-rogue-train-with-data-79405c86ab6a). The code here was written on November 5, 2016 - the actual day  when we were working on SMRT data to identify the cause of the Circle Line incidents. We acknowledge that there could be inefficiencies and welcome any comments and further sharing._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Authors: Lee Shangqian, Daniel Sim and Clarence Ng, Data Science Division, GovTech Singapore\n",
    "\n",
    "We were given a data dump containing the following information:\n",
    "- Date and time of of each incident\n",
    "- Location of incident\n",
    "- ID of train involved\n",
    "- Direction of train\n",
    "\n",
    "We started by cleaning the data.\n",
    "\n",
    "As usual, the first step is to import some useful python libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import xlrd\n",
    "import itertools as it\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Make data extracts smaller for online consumption\n",
    "pd.set_option(\"display.max_rows\", 5)\n",
    "\n",
    "# Suppress warnings for online consumption\n",
    "pd.options.mode.chained_assignment = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then extract the useful parts from the raw data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dfincidents_0 = pd.read_excel('CCL EVAC E-brake occurrences hourly update_mod.xlsx', sheetname='Aug Sep')\n",
    "dfincidents_1 = pd.read_excel('CCL EVAC E-brake occurrences hourly update_mod.xlsx', sheetname='Nov')\n",
    "\n",
    "# Incident data for Nov had different format\n",
    "dfincidents_1['Time'] = dfincidents_1['Time'].str.strip('hrs').str.strip(' ')\n",
    "dfincidents_1['Time'] = pd.to_datetime(dfincidents_1['Time'], format='%H%M').dt.time\n",
    "\n",
    "dfincidents = pd.concat([dfincidents_0, dfincidents_1])\n",
    "\n",
    "# Reset the index because they were concatenated from two data sources\n",
    "dfincidents.reset_index(inplace=True, drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>S/N</th>\n",
       "      <th>S/N.1</th>\n",
       "      <th>Date</th>\n",
       "      <th>Traffic Date</th>\n",
       "      <th>PV</th>\n",
       "      <th>Time</th>\n",
       "      <th>Bound</th>\n",
       "      <th>Station from</th>\n",
       "      <th>Station to</th>\n",
       "      <th>Event</th>\n",
       "      <th>Remarks</th>\n",
       "      <th>DateTime</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2016-08-28</td>\n",
       "      <td>2016-08-28</td>\n",
       "      <td>PV40</td>\n",
       "      <td>19:32:00</td>\n",
       "      <td>OT</td>\n",
       "      <td>KRG</td>\n",
       "      <td>ONH</td>\n",
       "      <td>EB</td>\n",
       "      <td>point track</td>\n",
       "      <td>2016-08-28 19:32:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2016-08-28</td>\n",
       "      <td>2016-08-28</td>\n",
       "      <td>PV53</td>\n",
       "      <td>19:39:00</td>\n",
       "      <td>OT</td>\n",
       "      <td>LBD</td>\n",
       "      <td>PPJ</td>\n",
       "      <td>EB</td>\n",
       "      <td>point track</td>\n",
       "      <td>2016-08-28 19:39:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>257</th>\n",
       "      <td>57</td>\n",
       "      <td>42</td>\n",
       "      <td>2016-11-04</td>\n",
       "      <td>2016-11-04</td>\n",
       "      <td>PV13</td>\n",
       "      <td>22:29:00</td>\n",
       "      <td>IT</td>\n",
       "      <td>SDM</td>\n",
       "      <td>MBT</td>\n",
       "      <td>EB</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2016-11-04 22:29:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>258</th>\n",
       "      <td>58</td>\n",
       "      <td>43</td>\n",
       "      <td>2016-11-05</td>\n",
       "      <td>2016-11-05</td>\n",
       "      <td>PV43</td>\n",
       "      <td>00:07:00</td>\n",
       "      <td>IT</td>\n",
       "      <td>TLB</td>\n",
       "      <td>HBF</td>\n",
       "      <td>EVAC</td>\n",
       "      <td>Withdrawal train, no pax on-board</td>\n",
       "      <td>2016-11-05 00:07:00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>259 rows × 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     S/N  S/N.1       Date Traffic Date    PV      Time Bound Station from  \\\n",
       "0      1      1 2016-08-28   2016-08-28  PV40  19:32:00    OT          KRG   \n",
       "1      2      2 2016-08-28   2016-08-28  PV53  19:39:00    OT          LBD   \n",
       "..   ...    ...        ...          ...   ...       ...   ...          ...   \n",
       "257   57     42 2016-11-04   2016-11-04  PV13  22:29:00    IT          SDM   \n",
       "258   58     43 2016-11-05   2016-11-05  PV43  00:07:00    IT          TLB   \n",
       "\n",
       "    Station to Event                            Remarks            DateTime  \n",
       "0          ONH    EB                        point track 2016-08-28 19:32:00  \n",
       "1          PPJ    EB                        point track 2016-08-28 19:39:00  \n",
       "..         ...   ...                                ...                 ...  \n",
       "257        MBT    EB                                NaN 2016-11-04 22:29:00  \n",
       "258        HBF  EVAC  Withdrawal train, no pax on-board 2016-11-05 00:07:00  \n",
       "\n",
       "[259 rows x 12 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def datetime_from_date_and_time(row):\n",
    "    \"\"\"\n",
    "    Combines the date column and time column into a single column\n",
    "    \"\"\"\n",
    "    d = row['Date']\n",
    "    t = row['Time']\n",
    "    \n",
    "    return datetime(\n",
    "        d.year, d.month, d.day,\n",
    "        t.hour, t.minute, t.second\n",
    "    )\n",
    "\n",
    "# Add DateTime to the data for easier visualization\n",
    "dfincidents['DateTime'] = dfincidents.apply(datetime_from_date_and_time, axis=1)\n",
    "dfincidents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We could not find any obvious answers in our initial exploratory analysis, so our next step was to incorporate multiple dimensions. Our intention was to plot the incidents on a Marey Chart.\n",
    "\n",
    "First, we converted the station names from their three-letter codes to a number:\n",
    "- Marina Bay to before Promenade: 0 to 1.5\n",
    "- Dhoby Ghaut to HarbourFront: 2 to 29\n",
    "\n",
    "If the incident occurred between two stations, it will be denoted as 0.5 + the lower of the two station numbers. For example, If an incident happened between HarbourFront (number 29) and Telok Blangah (number 28), the location will be “28.5”. This makes it easy for us to plot the points along the horizontal axis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "stations=(\"MRB,BFT,DBG,BBS,EPN,PMN,NCH,SDM,MBT,DKT,PYL,MPS,TSG,BLY,SER,\"\n",
    "          \"LRC,BSH,MRM,CDT,BTN,FRR,HLV,BNV,ONH,KRG,HPV,PPJ,LBD,TLB,HBF\").split(',')\n",
    "\n",
    "\n",
    "def loc_id(station1, station2 = None):\n",
    "    \"\"\"\n",
    "    Translates a 3-letter station code to a number,\n",
    "    or a pair of 3-letter station codes to a number.\n",
    "    Single stations are represented as whole numbers.\n",
    "    Locations between stations are represented with a .5.\n",
    "    \n",
    "    Example:\n",
    "    loc_id('MRB')         # 0 (Marina Bay)\n",
    "    loc_id('MRB', 'BFT')  # 0.5 (Between Marina Bay and Bayfront)\n",
    "    loc_id('DBG')         # 2 (Dhoby Ghaut)\n",
    "    loc_id('HBF')         # 29\n",
    "    loc_id('HBF', 'TLB')  # 28.5 (Between Harbourfront and Telok Blangah)\n",
    "    loc_id('HBF', 'DBG')  # throws and error, because these stations are not adjacent\n",
    "    \"\"\"\n",
    "    if station2 == None or station2 == 'nan' or (type(station2) is float and math.isnan(station2)):\n",
    "        # Single stations\n",
    "        return stations.index(station1)\n",
    "    \n",
    "    else: # Pairs of stations -- take the average to get the 0.5\n",
    "        stn1_index = stations.index(station1)\n",
    "        stn2_index = stations.index(station2)\n",
    "        \n",
    "        # Handle the branch at Promenade\n",
    "        if (set(['PMN', 'EPN']) == set([station1, station2])):\n",
    "            return float(stations.index('EPN')) + 0.5\n",
    "        elif set(['PMN', 'BFT']) == set([station1, station2]):\n",
    "            return float(stations.index('BFT')) + 0.5\n",
    "        else:\n",
    "            # Require station pairs to be adjacent stations\n",
    "            assert(math.fabs(stn1_index - stn2_index) == 1)\n",
    "            return float(stn1_index + stn2_index) / 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And then we computed the numeric location IDs…"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def loc_id_from_stations(row):\n",
    "    try:\n",
    "        # This handles entries with both \"Station from\" and \"Station to\"\n",
    "        # and entries with only \"Station from\"\n",
    "        return loc_id(row['Station from'], str(row['Station to']))\n",
    "    except ValueError:\n",
    "        # Some entries only have \"Station to\" but no\n",
    "        # \"Station from\"\n",
    "        return loc_id(row['Station to'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And added that to the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>DateTime</th>\n",
       "      <th>PV</th>\n",
       "      <th>Bound</th>\n",
       "      <th>Station from</th>\n",
       "      <th>Station to</th>\n",
       "      <th>Event</th>\n",
       "      <th>Remarks</th>\n",
       "      <th>LocID</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2016-08-28 19:32:00</td>\n",
       "      <td>PV40</td>\n",
       "      <td>OT</td>\n",
       "      <td>KRG</td>\n",
       "      <td>ONH</td>\n",
       "      <td>EB</td>\n",
       "      <td>point track</td>\n",
       "      <td>23.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2016-08-28 19:39:00</td>\n",
       "      <td>PV53</td>\n",
       "      <td>OT</td>\n",
       "      <td>LBD</td>\n",
       "      <td>PPJ</td>\n",
       "      <td>EB</td>\n",
       "      <td>point track</td>\n",
       "      <td>26.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>257</th>\n",
       "      <td>2016-11-04 22:29:00</td>\n",
       "      <td>PV13</td>\n",
       "      <td>IT</td>\n",
       "      <td>SDM</td>\n",
       "      <td>MBT</td>\n",
       "      <td>EB</td>\n",
       "      <td>NaN</td>\n",
       "      <td>7.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>258</th>\n",
       "      <td>2016-11-05 00:07:00</td>\n",
       "      <td>PV43</td>\n",
       "      <td>IT</td>\n",
       "      <td>TLB</td>\n",
       "      <td>HBF</td>\n",
       "      <td>EVAC</td>\n",
       "      <td>Withdrawal train, no pax on-board</td>\n",
       "      <td>28.5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>259 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               DateTime    PV Bound Station from Station to Event  \\\n",
       "0   2016-08-28 19:32:00  PV40    OT          KRG        ONH    EB   \n",
       "1   2016-08-28 19:39:00  PV53    OT          LBD        PPJ    EB   \n",
       "..                  ...   ...   ...          ...        ...   ...   \n",
       "257 2016-11-04 22:29:00  PV13    IT          SDM        MBT    EB   \n",
       "258 2016-11-05 00:07:00  PV43    IT          TLB        HBF  EVAC   \n",
       "\n",
       "                               Remarks  LocID  \n",
       "0                          point track   23.5  \n",
       "1                          point track   26.5  \n",
       "..                                 ...    ...  \n",
       "257                                NaN    7.5  \n",
       "258  Withdrawal train, no pax on-board   28.5  \n",
       "\n",
       "[259 rows x 8 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Select only some columns that we are interested in\n",
    "sel_dfincidents = dfincidents[['DateTime', 'PV', 'Bound', 'Station from', 'Station to', 'Event', 'Remarks']]\n",
    "\n",
    "# Add the location ID into the dataset\n",
    "sel_dfincidents['LocID'] = sel_dfincidents.apply(loc_id_from_stations, axis=1)\n",
    "sel_dfincidents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the data processed, we were able to create a scatterplot of all the emergency braking incidents. \n",
    "\n",
    "We noticed that the sequence of breakdowns seem to move “backwards”. In other words, when a train gets hit by the interference, another train behind moving in the same direction gets hit soon after."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What moves backwards in a tunnel? \n",
    "\n",
    "At this point, it still wasn’t clear that a single train was the culprit. \n",
    "\n",
    "But what’s the most obvious thing that moves down a train line along a tunnel? Could it be a train moving in the opposite direction? We decided to test this “rogue train” hypothesis.\n",
    "\n",
    "We knew that the travel time between stations along the Circle Line ranges between two and four minutes. This means we could group all emergency braking incidents together if they occur up to four minutes apart."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def same_cascade(i, j):\n",
    "    \"\"\"\n",
    "    Given a pair of incidents (i,j), returns true if:\n",
    "    \n",
    "        t <= d * 4 mins\n",
    "    \n",
    "    where t is  the time difference between occurrences\n",
    "    and d is the distance (measured by difference in location ID).\n",
    "    \n",
    "    Moreover, we consider the track direction, and only consider\n",
    "    incidents that are \"moving backwards\".\n",
    "    \"\"\"\n",
    "    \n",
    "    # If trains are not travelling in the same direction\n",
    "    # they cannot be due to the same \"backward moving\" interference\n",
    "    # source.\n",
    "    # (Note: This was the hypothesis when this code was written.\n",
    "    # It turned out that the rogue train could affect all\n",
    "    # trains in the vicinity, not just in the opposite track)\n",
    "    if i[\"Bound\"] != j[\"Bound\"] or \\\n",
    "        i[\"Bound\"] not in ['IT', 'OT']:\n",
    "        return False\n",
    "    \n",
    "    # time difference in minutes\n",
    "    time_difference = (i[\"DateTime\"] - j[\"DateTime\"]) / np.timedelta64(1, 'm')\n",
    "    location_difference = i[\"LocID\"] - j[\"LocID\"]\n",
    "    \n",
    "    if location_difference == 0:\n",
    "        return False\n",
    "    \n",
    "    ratio = time_difference / location_difference\n",
    "    \n",
    "    if i[\"Bound\"] == 'OT':\n",
    "        return ratio > 0 and ratio < 4\n",
    "    elif i[\"Bound\"] == 'IT':\n",
    "        return ratio < 0 and ratio > -4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We found all incident pairs that satisfied this condition:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "incidents = sel_dfincidents.to_records()\n",
    "# (a, b, c, d, ...) --> ((a,b), (a,c), (a,d), ..., (b,c), (b,d), ..., (c,d), ...)\n",
    "incident_pairs = list(it.combinations(incidents, 2))\n",
    "\n",
    "related_pairs = [ip for ip in incident_pairs if same_cascade(*ip)]\n",
    "related_pairs = [(i[0], j[0]) for i,j in related_pairs]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then grouped all related pairs of incidents into larger sets using a disjoint-set data structure. This allows us to group incidents that could be linked to the same “rogue train”."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def pairs_to_clusters(pairs):\n",
    "    \"\"\"\n",
    "    A quick-and-dirty disjoint-set data structure. \n",
    "    But this works fast enough for 200+ records.\n",
    "    Could be better.\n",
    "    \n",
    "    Example input:\n",
    "    (1,2), (2,3), (4,5)\n",
    "    \n",
    "    Output:\n",
    "    1: {1,2,3}\n",
    "    2: {1,2,3}\n",
    "    3: {1,2,3}\n",
    "    4: {4,5}\n",
    "    5: {4,5}\n",
    "    \"\"\"\n",
    "    the_clusters = dict()\n",
    "\n",
    "    for i,j in pairs:\n",
    "        if i not in the_clusters:\n",
    "            if j in the_clusters:\n",
    "                the_clusters[j].add(i)\n",
    "                the_clusters[i] = the_clusters[j]\n",
    "            else:\n",
    "                the_clusters[i] = set(list([i, j]))\n",
    "                the_clusters[j] = the_clusters[i]\n",
    "        else:\n",
    "            if j in the_clusters:\n",
    "                if the_clusters[i] is not the_clusters[j]: # union the two sets\n",
    "                    for k in the_clusters[j]:\n",
    "                        the_clusters[i].add(k)\n",
    "                        the_clusters[k] = the_clusters[i]\n",
    "                else: # they are already in the same set\n",
    "                    pass\n",
    "            else:\n",
    "                the_clusters[i].add(j)\n",
    "                the_clusters[j] = the_clusters[i]\n",
    "    \n",
    "    return the_clusters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we applied our algorithm to the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{0, 1},\n",
       " {2, 4},\n",
       " {5, 6, 7},\n",
       " {8, 9},\n",
       " {18, 19, 20},\n",
       " {21, 22, 24, 26, 27},\n",
       " {28, 29, 30, 31, 32, 33, 34},\n",
       " {42, 44, 45},\n",
       " {47, 48},\n",
       " {51, 52, 53, 56}]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clusters = pairs_to_clusters(related_pairs)\n",
    "# Show each set only once\n",
    "clusters = [v for k,v in clusters.items() if min(v) == k]\n",
    "clusters[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we calculated the percentage of the incidents that can explained by our clustering algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(189, 259, 0.7297297297297297)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# count % of incidents occurring in a cluster\n",
    "all_clustered_incidents = set()\n",
    "\n",
    "for i,clust in enumerate(clusters):\n",
    "    all_clustered_incidents |= clust\n",
    "\n",
    "(len(all_clustered_incidents),\n",
    " len(incidents),\n",
    " float(len(all_clustered_incidents)) / len(incidents))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What it means: Of the 259 emergency braking incidents in our dataset, 189 cases — or 73% of them — could be explained by the “rogue train” hypothesis.\n",
    "\n",
    "We went on to process the historical location data of trains on the Circle Line and concluded that more than 95% of incidents from August to November could be explained by the rogue train hypothesis. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
